{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation and Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a situation in which we do not have 25,000 pictures to train. And not even 3000. And there are only 300. In such situations, data augmentation is useful. We will also use fine-tuning to improve the quality of the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of Required Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files = '../datasets/transfer_learning/train'\n",
    "\n",
    "images = [os.path.join(dp, f) for dp, dn, filenames \n",
    "          in os.walk(path_to_files) for f in filenames\n",
    "          if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return img, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CAT_SIZE = 150\n",
    "AUG_NUM = 9\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "data = []\n",
    "cats_counter = 0\n",
    "dogs_counter = 0\n",
    "for img_path in images:\n",
    "    category = img_path.split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "    if category == 'dog' and dogs_counter < MAX_CAT_SIZE:\n",
    "        img, x = get_image(img_path)\n",
    "        data.append({'x':np.array(x[0]), 'y':0})\n",
    "        \n",
    "        for _ in range(AUG_NUM):\n",
    "            aug_x = data_augmentation(x)\n",
    "            data.append({'x':np.array(aug_x[0]), 'y':0})\n",
    "            \n",
    "        dogs_counter += 1\n",
    "    elif category == 'cat' and cats_counter < MAX_CAT_SIZE:\n",
    "        img, x = get_image(img_path)\n",
    "        data.append({'x':np.array(x[0]), 'y':1})\n",
    "        \n",
    "        for _ in range(AUG_NUM):\n",
    "            aug_x = data_augmentation(x)\n",
    "            data.append({'x':np.array(aug_x[0]), 'y':1})\n",
    "        \n",
    "        cats_counter += 1\n",
    "    elif dogs_counter == MAX_CAT_SIZE and cats_counter == MAX_CAT_SIZE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "train_split = 0.8\n",
    "idx_test = int((train_split) * len(data))\n",
    "\n",
    "data_train = data[:idx_test]\n",
    "data_test = data[idx_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = np.array([t[\"x\"] for t in data_train]), [t[\"y\"] for t in data_train]\n",
    "x_test, y_test = np.array([t[\"x\"] for t in data_test]), [t[\"y\"] for t in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# convert labels to one-hot vectors\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 + Classification Layer + Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a reference to VGG's input layer\n",
    "inp = vgg.input\n",
    "\n",
    "# make a new softmax layer with num_classes neurons\n",
    "new_classification_layer = Dense(num_classes, activation='softmax')\n",
    "\n",
    "# connect our new layer to the second to last layer in VGG, and make a reference to it\n",
    "out = new_classification_layer(vgg.layers[-2].output)\n",
    "\n",
    "# create a new network between inp and out\n",
    "model = Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 134,268,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune at this layer\n",
    "fine_tune_at = 2\n",
    "\n",
    "for l, layer in enumerate(model.layers[:-1]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# ensure these layers is trainable/not frozen\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[fine_tune_at].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 45,122\n",
      "Non-trainable params: 134,223,616\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "48/48 [==============================] - 748s 16s/step - loss: 0.6466 - accuracy: 0.6722\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 754s 16s/step - loss: 0.2811 - accuracy: 0.9112\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 746s 16s/step - loss: 0.2029 - accuracy: 0.9338\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 740s 15s/step - loss: 0.1643 - accuracy: 0.9553\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 737s 15s/step - loss: 0.1336 - accuracy: 0.9554\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 741s 15s/step - loss: 0.1231 - accuracy: 0.9580\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 737s 15s/step - loss: 0.1037 - accuracy: 0.9669\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 738s 15s/step - loss: 0.0918 - accuracy: 0.9756\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 737s 15s/step - loss: 0.0850 - accuracy: 0.9760\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 739s 15s/step - loss: 0.0752 - accuracy: 0.9828\n",
      "Wall time: 2h 3min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(x_train, y_train, batch_size=50, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07731444388628006\n",
      "Test accuracy: 0.9750000238418579\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final accuracy on test data: 97.5%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.jpg - dog picture\n",
    "category = 0 \n",
    "img, x = get_image('../datasets/transfer_learning/test/2.jpg')\n",
    "probabilities = model.predict([x])\n",
    "category == probabilities.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.jpg - cat picture\n",
    "category = 1\n",
    "img, x = get_image('../datasets/transfer_learning/test/7.jpg')\n",
    "probabilities = model.predict([x])\n",
    "category == probabilities.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../keras_models/vgg_model_with_aug_and_tun\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../keras_models/vgg_model_with_aug_and_tun')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
